"""
Test Generator Agent

Responsibility: Generate MCQ assessments based on job requirements.
Single purpose: Create fair, relevant test questions aligned to the JD.

This agent does NOT evaluate responses - only generates questions.
"""

from dataclasses import dataclass
from typing import Any, Dict, List

from .base import BaseAgent
from ..schemas.job import ParsedJD, TestQuestion


@dataclass
class TestGeneratorInput:
    """Input for test generation."""
    parsed_jd: ParsedJD
    num_questions: int = 20
    difficulty_distribution: Dict[str, float] = None  # e.g., {"easy": 0.3, "medium": 0.5, "hard": 0.2}
    
    def __post_init__(self):
        if self.difficulty_distribution is None:
            self.difficulty_distribution = {"easy": 0.3, "medium": 0.5, "hard": 0.2}


@dataclass
class TestGeneratorOutput:
    """Output from test generation."""
    test_id: str
    job_id: str
    questions: List[TestQuestion]
    total_time_minutes: int
    topics_covered: List[str]
    difficulty_breakdown: Dict[str, int]


class TestGeneratorAgent(BaseAgent[TestGeneratorInput, TestGeneratorOutput]):
    """
    Generates MCQ assessments based on job description requirements.
    
    Input: ParsedJD + generation parameters
    Output: List of test questions with answers and explanations
    
    Key responsibilities:
    - Generate questions aligned to required skills
    - Ensure fair difficulty distribution
    - Provide clear explanations for answers
    - Avoid culturally biased questions
    
    Does NOT:
    - Evaluate candidate responses
    - Score tests
    - Make hiring decisions
    """
    
    @property
    def description(self) -> str:
        return (
            "Generates fair, job-relevant MCQ assessments from parsed job "
            "descriptions, ensuring coverage of required skills and topics."
        )
    
    @property
    def required_confidence_threshold(self) -> float:
        return 0.75  # Questions need review before use
    
    def _process(
        self, 
        input_data: TestGeneratorInput
    ) -> tuple[TestGeneratorOutput, float, str]:
        """
        Generate test questions for a job.
        
        Args:
            input_data: TestGeneratorInput with JD and parameters
        
        Returns:
            TestGeneratorOutput, confidence_score, explanation
        """
        jd = input_data.parsed_jd
        num_questions = input_data.num_questions
        
        self.log_reasoning(f"Generating {num_questions} questions for job {jd.job_id[:8]}")
        self.log_reasoning(f"Topics to cover: {jd.technical_topics}")
        
        # TODO: Implement actual question generation with LLM
        # For now, return mock output
        
        from uuid import uuid4
        test_id = uuid4().hex
        
        # Calculate questions per difficulty
        diff_dist = input_data.difficulty_distribution
        difficulty_breakdown = {
            level: int(num_questions * ratio)
            for level, ratio in diff_dist.items()
        }
        
        # Mock questions
        questions = []
        for i in range(min(3, num_questions)):  # Just 3 mock questions
            q = TestQuestion(
                job_id=jd.job_id,
                question_text=f"[Mock Question {i+1}] - To be generated by LLM",
                options={
                    "A": "Option A",
                    "B": "Option B",
                    "C": "Option C",
                    "D": "Option D",
                },
                correct_option="A",
                explanation="Explanation to be generated",
                skill_tested=jd.technical_topics[0] if jd.technical_topics else "general",
                topic=jd.technical_topics[0] if jd.technical_topics else "general",
                difficulty="medium",
            )
            questions.append(q)
        
        output = TestGeneratorOutput(
            test_id=test_id,
            job_id=jd.job_id,
            questions=questions,
            total_time_minutes=num_questions * 2,  # 2 min per question
            topics_covered=jd.technical_topics,
            difficulty_breakdown=difficulty_breakdown,
        )
        
        confidence = 0.8
        explanation = (
            f"Generated test with {len(questions)} questions (mock). "
            f"Covers {len(jd.technical_topics)} topics. "
            f"Estimated time: {output.total_time_minutes} minutes."
        )
        
        self.log_reasoning("Test generation completed")
        
        return output, confidence, explanation
